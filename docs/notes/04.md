---
layout: notes
title: Deep Learning - 04 - Convolutional Neural Networks
---

# Chapter 4: Convolutional Neural Networks

## Fully Connected Layers and Images

Say we want to run our neural networks on image data. We know that in image processing, feature extraction is very important to make machine learning models work well.

One standard technique for extracting features is the **histogram of oriented gradients (HOG)** method. You don't have to worry too much about how HOG works, it's basically just looking at patches in the input image and measuring which way the gradients (edges) face in those patches. The important part is the dimensions of HOG features. HOG features produce a 36-element vector for each $$8 \x 8$$ patch in the input.

Say we want to learn feature extraction using neural networks instead. If we want to mimic HOG features our network might look like:
- Input: $$ 256 \x 256 \x 3 $$ RGB image
- Hidden: $$ 32 \x 32 \x 36 $$ feature map
- Output: $$ 1000 $$ classes

Since fully connected layers are _fully_ connected there are 7.2 billion connections between the input and hidden layer!

During feature extraction, our network is looking at the entire image every time it computes a single feature. However, HOG features (and most image features) are meant to capture local information, like what edges are present nearby. We don't need to look at the whole image for this.

For most feature extraction on images we can impose the assumption that pixels that are far apart are statistically independent. This means we only need to look at smaller, local regions when performing feature extraction.

## Convolutions: Powerful, Local Feature Extraction

**Convolutions** are a powerful tool for local feature extraction in a signal. The convolution operation takes two functions and slides one along the other, performing a dot product of their intersection to form an output signal. Note: in computer vision we talk about convolutions but in the signal processing domain what we're actually talking about is a cross-correlation. I'm not going to try to correct a decade of mistaken terminology in this class so I'm just going to call it a convolution idk sorry...

Anyway, it's nice to start with a 1-D example. We'll be talking about discrete functions which we can just think of as vectors. We'll define the convolution operator as follows:

$$(m * f)(x) = m_{[x-1, x+1]} \cdot f $$

### Example Convolution: High-Pass Filter

Take discrete functions $$m$$ and $$f$$. To convolve these filters we simply slide the filter $$f$$ along input signal (or function) $$m$$ and take weighted sums, storing the result in a new vector.

- Let's calculate $$m * f$$ where 
    - \\(m = [0, 1, 2, 3, 2, 5, 6, 7, 8, 14, 10, 11, 12, 8, 14]\\)
    - \\(f = [-1, 2, -1]\\)
    - First element is dot product of first three elements of $$m$$ with $$f$$, $$m_{[0-2]} \cdot f$$
    - ...

$$
\begin{align*}
&(m*f)(1)  = m_{[0-2]} \cdot f  = [0,1,2] \cdot [-1,2,-1]      =   0 + 2 - 2     =& 0   \\
&(m*f)(2)  = m_{[1-3]} \cdot f  = [1,2,3] \cdot [-1,2,-1]      =  -1 + 4 - 3     =& 0   \\
&(m*f)(3)  = m_{[2-4]} \cdot f  = [2,3,2] \cdot [-1,2,-1]      =  -2 + 6 - 2     =& 2   \\
&(m*f)(4)  = m_{[3-5]} \cdot f  = [3,2,5] \cdot [-1,2,-1]      =  -3 + 4 - 5     =& -4  \\
&(m*f)(5)  = m_{[4-6]} \cdot f  = [2,5,6] \cdot [-1,2,-1]      =  -2 + 10 - 6    =& 2   \\
&(m*f)(6)  = m_{[5-7]} \cdot f  = [5,6,7] \cdot [-1,2,-1]      =  -5 + 12 - 7    =& 0   \\
&(m*f)(7)  = m_{[6-8]} \cdot f  = [6,7,8] \cdot [-1,2,-1]      =  -6 + 14 - 8    =& 0   \\
&(m*f)(8)  = m_{[6-8]} \cdot f  = [7,8,14] \cdot [-1,2,-1]     =  -7 + 16 - 14   =& -5  \\
&(m*f)(9)  = m_{[6-8]} \cdot f  = [8,14,10] \cdot [-1,2,-1]    =  -8 + 28 - 10   =& 10  \\
&(m*f)(10) = m_{[6-8]} \cdot f  = [14,10,11] \cdot [-1,2,-1]   =  -14 + 20 - 11  =& -5  \\
&(m*f)(11) = m_{[6-8]} \cdot f  = [10,11,12] \cdot [-1,2,-1]   =  -10 + 22 - 12  =& 0   \\
&(m*f)(12) = m_{[6-8]} \cdot f  = [11,12,8] \cdot [-1,2,-1]    =  -11 + 24 - 8   =& 5   \\
&(m*f)(13) = m_{[6-8]} \cdot f  = [12,8,14] \cdot [-1,2,-1]    =  -12 + 16 - 14  =& -10 \\
\end{align*}
$$

So the output is the vector:

$$m*f = [0 , 0 , 2 , -4 , 2 , 0 , 0 , -5 , 10 , -5 , 0 , 5 , -10]$$


This particular filter, $$f = [-1, 2, -1] $$, is a **high-pass filter**. It doesn't respond to slow changes in the function but does respond to fast changes. The results is an output signal centered around zero that oscillates up and down when the input signal quickly moves up or down.

{% include figure
    html='<iframe src="https://www.desmos.com/calculator/6tm5gtkaxs?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>'
    caption="Input signal \(m\) and output \(m*f\). The high-pass filter suppresses the slow rise of \(m\) but shows the wiggles."
%}

### Padding

Notice that our input signal $$m$$ has 15 elements but our output $$m*f$$ only has 13 elements. We can't apply the filter outside the bounds of our original function $$m$$ so our output signal $$m*f$$ with have size $$\vert m\vert - \lfloor\frac{\vert f \vert}{2}\rfloor$$, in this case $$ 15 - 2 = 13$$. Often it is helpful to have your input and output be the same size (like when processing data with neural networks!) so you may want to **pad** your input with data to make the output end up the same size. Consider using:

- **zero-padding**: pad with zeros
- **clamp-padding** (extend/replicate padding): repeat the last elements outward
- **wrap-padding** (circular padding): wrap around to the other side of the input signal
- **mirror-padding** (reflect padding): reflect data from inside the signal outward into padded region

Most of the time zero-padding is a good option but you can try other versions too.

### Low-Pass Filter

Another useful filter to know is $$f = [0.\overline{3}, 0.\overline{3}, 0.\overline{3}]$$ or $$f = [0.2, 0.2, 0.2, 0.2, 0.2]$$ or in general $$f = \frac{1}{n}[1,1,1,\dots,1]$$. This is a **low-pass filter** or blurring filter, the output at every pixel is an average of a local window in the input signal.

### Convolutions In 2D

Convolutional filters are powerful feature extractors in 2D. Here's an example of high-pass and low-pass filters (or **box filter**) in two dimensions:

$$
\text{High-Pass Filter} = \begin{bmatrix}
-1 & -1 & -1 \\
-1 &  8 & -1 \\
-1 & -1 & -1 \\
\end{bmatrix}$$

$$
\text{Low-Pass "Box" Filter} = \frac{1}{9}\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix}$$

These fil
