---
layout: slides
title: Deep Learning - 09 - Recurrent Neural Networks
premalink: /slides/09
---
class: center, middle

# Chapter 9 - Recurrent Neural Networks


---

# COCO dataset has captions!

.col50[![4 images from the COCO dataset with example captions. Top left image shows a baseball game with the caption: "The man at bat readies to swing at the pitch while the umpire looks on". Top right image shows a bus station with the caption: "A large bus sitting next to a very tall building". Bottom left shows a horse pulling a cart with the caption: "A horse carrying a large load of hay and two people sitting on it". Bottom right image shows a bedroom with the caption: "Bunk bed with a narrow shelf sitting underneath it". ](../figs/coco.png)]
.col50[
- 5 captions per image
- Detection/segmentation is (maybe) just pattern matching
- Captioning requires _understanding_
- Harder: have to model both visual info and language
]

---

# Natural language processing using neural networks

- Images are static, language has a component of time
- Characters/words appear in sequence, need to read previous words to understand subsequent ones (like frames in a video)
- How do we process time-series data using neural networks?

---

class: center, middle
# A long time ago in a \_\_\_\_\_\_ far, far away...

---
class: center, middle
# A long time ago in a galaxy far, far away...

---
# Neural networks + language

.col70[
- Naive approach
    - Input: string
    - Output: string
- How would we encode?
    - ~3000 common words
- 1-hot encoding of all 6-word strings: 
    - 18,000 possible inputs
    - small vocabulary
    - how to handle 7-word or 8-word strings?
    - retrain network? what about longer output (multi-word)?
    ]

.col30.small[
{% include chart
chart='
graph TD
    string(A long time ago in a) --> |encode| input[Input]
    subgraph Neural Network
    input --> hidden[Hidden Layer]
    hidden --> output[Output]
    end
    output --> |decode| outstring(galaxy)
'
caption="Naive network architecture for processing language data."
%}
]

---

# Recurrent neural network

- Handle sequential data
- How?
- During each iteration network:
    - Read one token (word, character, etc) at a time
    - Produce output
    - Update internal memory

{% include image
    src="../figs/rnnstarwars.png"
    alt="RNN example on sentence. Network processes input words to produce output but also hidden layers of the network are connected forward in time to the next word. Network tries to predict next word in sentence based on the current word. Input words are [A, long, time, ago, in], output from network is [long, time, ago, in, a]."
    attribution=""
    caption=""
%}

---
# Recurrent neural network

.col50[- Handle sequential data
- How?
- During each iteration network:
    - Read one token (word, character, etc) at a time
    - Produce output
    - Update internal memory
]
.col50[
{% include chart
chart='
graph TB
    Input --> Memory
    Memory --> Output
    Memory --> |Update Rule| Memory
'
caption="Recurrent network, memory gets updated from the input and previous time-step's memory, then produces output."
%}
]

---
# Vanilla RNN

.col50[
Given input \\(x\_t\\), previous memory \\(h\_{t-1}\\), produce output \\(y\_t\\)

- \\(h\_t = f(w\cdot x\_t + v \cdot h\_{t-1}) \\)
- \\(y\_t = h\_t \\)

Note that:
- output is same as current memory
- \\(w\\) takes input and updates memory
- \\(v\\) takes previous memory and updates current
- \\(f\\) is some activation function
]
.col50[
{% include chart
chart='
graph TB
    Input["\(x_t\)"] --> |"\(w\)"| Memory["\(h_t\)"]
    Memory --> Output["\(y_t\)"]
    Memory --> |"\(v\)"| Memory
'
caption="Example of vanilla RNN structure."
%}
]

---
# Vanilla RNN

.col50[
Given input \\(x\_t\\), previous memory \\(h\_{t-1}\\), produce output \\(y\_t\\)

In practice, append \\(x\_t\\) to \\(h\_{t-1}\\) and use one set of weights

- \\(h\_t = f(w\cdot [x\_t, h\_{t-1}]) \\)
- \\(y\_t = h\_t \\)
]
.col50[
{% include chart
chart='
graph TB
    Input["\(x_t\)"] --> |"\(w\)"| Memory["\(h_t\)"]
    Memory --> Output["\(y_t\)"]
    Memory --> |"\(v\)"| Memory
'
caption="Example of vanilla RNN structure."
%}
]

---
# Vanilla RNN

.col50[
Given input \\(x\_t\\), previous memory \\(h\_{t-1}\\), produce output \\(y\_t\\)

In practice, append \\(x\_t\\) to \\(h\_{t-1}\\) and use one set of weights

- \\(h\_t = f(w\cdot [x\_t, h\_{t-1}]) \\)
- \\(y\_t = h\_t \\)

Problem: long-term dependence
- memory computed from scratch every round
- hard to remember things for a long time
- memory state after 5-6 words will be VERY different even though it is the same sentence, paragraph, etc
- want longer term memory!!
]
.col50[
{% include chart
chart='
graph TB
    Input["\(x_t\)"] --> |"\(w\)"| Memory["\(h_t\)"]
    Memory --> Output["\(y_t\)"]
    Memory --> |"\(v\)"| Memory
'
caption="Example of vanilla RNN structure."
%}
]

---
# Idea: incremental change to memory

- Instead of completely re-doing memory every round, write to and read from memory like a computer (sort of)
- Calculate gating function to decide what parts of memory to keep and what to change
- Allows network to make changes but also remember important info longer term.

Different options:
- Gated recurrent units (GRU)
- Long short-term memory (LSTM)
- others...

---
# GRU: gated recurrent units






{% include chart
chart='
graph LR

    s1(A) --> |encode| in1[Input]
    subgraph Time Step 1
    in1 --> hidden1[Hidden Layer]
    hidden1 --> out1[Output]
    end
    out1 --> |decode| outs1(long)

    s2(long) --> |encode| in2[Input]
    subgraph Time Step 2
    in2 --> hidden2[Hidden Layer]
    hidden2 --> out2[Output]
    end
    out2 --> |decode| outs2(time)
    hidden1[Hidden Layer] --> hidden2[Hidden Layer]

    s3(time) --> |encode| in3[Input]
    subgraph Time Step 3
    in3 --> hidden3[Hidden Layer]
    hidden3 --> out3[Output]
    end
    out3 --> |decode| outs3(ago)
    hidden2[Hidden Layer] --> hidden3[Hidden Layer]

    s4(ago) --> |encode| in4[Input]
    subgraph Time Step 4
    in4 --> hidden4[Hidden Layer]
    hidden4 --> out4[Output]
    end
    out4 --> |decode| outs4(in)
    hidden3[Hidden Layer] --> hidden4[Hidden Layer]



'
caption="Naive network architecture for processing language data."
%}

---


# Math Test

$$\frac{d}{dx} L_2(\theta \mid w, x)$$

{% include chart
chart='
graph TB
    c1-->a2
    subgraph one
    a1-->a2
    end
    subgraph two
    b1-->b2
    end
    subgraph three
    c1-->c2
    end

'
caption="Naive network architecture for processing language data."
%}

---

